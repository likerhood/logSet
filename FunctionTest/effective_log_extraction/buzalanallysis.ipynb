{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总日志分析\n",
    "## 1. 总日志txt特点\n",
    "- `bugreport-`开头\n",
    "- 大约两百万行日志\n",
    "- 包含唤醒方案等相关文字信息\n",
    "- 包含其他系统信息\n",
    "## 2. 需求分析\n",
    "1. 可以得到唤醒方案、算法、机型等数据；\n",
    "2. 可以分析每一行日志，但是时间比较长；\n",
    "3. 可以筛选出某一时刻在某一时间内的全部有效日志；\n",
    "    - 全部有效日志是指包含voicetrigger项目中关键log打印的关键词；\n",
    "    - 全部关键词放在一个json文件中；\n",
    "    - 难点：\n",
    "        **如何分析和展示某一时刻附近发生的问题？**\n",
    "        1. 快速获得有效时间内的全部日志（前后7分钟内日志大约30万行）\n",
    "        1. 分析该时刻前后7分钟的全部日志，得出附近的一级问题和二级问题；\n",
    "        2. 交给用户来确定分析该时刻的附近几分钟（也可以考虑）；\n",
    "## 3. 难点\n",
    "1. ~二分法快速锁定某一范围的日志（筛选的日志有遗漏，需要其他优化方法）~\n",
    "    1. 正则匹配确定时间\n",
    "        OK\n",
    "    2. 二分法确定范围\n",
    "        OK\n",
    "    3. 正则匹配 + 有效日志关键词筛选得到最终有效日志\n",
    "        OK\n",
    "\n",
    "2. 对于不是完全按照时间顺序的日志，如何快速筛选一段时间内的有效日志\n",
    "    **速度上可以继续优化**\n",
    "    1. 直接每次分析每2000行的第一条和最后一条日志的时间。如果其中一条日志满足，就直接将这1000行日志最为有效日志存起来；\n",
    "    2. 对于遍历大数量的日志确实能提高效率，比逐行遍历每一条日志提高效率：\n",
    "        1. 逐行遍历时间复杂度为`O(n)`\n",
    "        2. 间隔2000行粗略筛选，时间复杂度为`O(n/2000)`\n",
    "        3. 为什么是选择间隔1000行。因为我发现一段有效且连续在一分钟内的日志一定大于2000行。\n",
    "    3. 粗略筛选之后还需要进行第二次细致筛选:\n",
    "        正则匹配 + 有效日志关键词筛选得到最终有效日志\n",
    "\n",
    "\n",
    "## 4. 日志问题关键词匹配\n",
    "1. 得到唤醒方案、机型等文字信息\n",
    "    TODO:貌似需要分析全部日志，个人认为可以通过后续在导出的表格中直接获取，没必要浪费时间分析全部日志\n",
    "    1. voice_trigger_version\n",
    "    关键日志：\n",
    "    ```\n",
    "    # voice_trigger_version\n",
    "    Package [com.miui.voicetrigger] (8c57a04):\n",
    "    appId=10181\n",
    "    pkg=Package{a5a5bed com.miui.voicetrigger}\n",
    "    codePath=/product/app/VoiceTrigger\n",
    "    resourcePath=/product/app/VoiceTrigger\n",
    "    legacyNativeLibraryDir=/product/app/VoiceTrigger/lib\n",
    "    extractNativeLibs=true\n",
    "    primaryCpuAbi=arm64-v8a\n",
    "    secondaryCpuAbi=null\n",
    "    cpuAbiOverride=null\n",
    "    versionCode=2024060310 minSdk=26 targetSdk=33\n",
    "    minExtensionVersions=[]\n",
    "    versionName=v-5.9.0.11-qcom\n",
    "    ```\n",
    "    关键代码\n",
    "    ```python\n",
    "    if not is_get_package_info and re.search('Package \\[com.miui.voicetrigger\\]', self.log_lines[i]) is not None:\n",
    "        self.build_date = self.log_lines[i+10].split('=')[1]\n",
    "        self.voice_trigger_version = self.log_lines[i+12].split('=')[1:]\n",
    "        is_get_package_info = True\n",
    "    ```\n",
    "\n",
    "\n",
    "2. 未收到一级唤醒事件\n",
    "四种方案通用\n",
    "未查询到关键日志点‘onRecognition:’，则标记为：未接收到一级事件\n",
    "```bash\n",
    "(高通/自研E) D/SVA-LegacyWakeupSession - onRecognition: recognitionEvent = KeyphraseRecognitionEvent [keyphraseExtras=[KeyphraseRecognitionExtra [id=0, recognitionModes=3, coarseConfidenceLevel=98, confidenceLevels=[ConfidenceLevel [userId=1, confidenceLevel=14]]]], status=0, soundModelHandle=1, captureAvailable=true, captureSession=25, captureDelayMs=0, capturePreambleMs=0, triggerInData=false, sampleRate=16000, encoding=2, channelMask=16, data=8944]\n",
    "(MTK) V/VoiceTriggerManager - onRecognition: GenericRecognitionEvent ::RecognitionEvent [status=0, soundModelHandle=-1, captureAvailable=false, captureSession=-1, captureDelayMs=-1, capturePreambleMs=-1, triggerInData=false, sampleRate=16000, encoding=2, channelMask=1, data=0]\n",
    "(MTK自研) V/MTKVoiceTriggerManager - onRecognition: GenericRecognitionEvent ::RecognitionEvent [status=0, soundModelHandle=-1, captureAvailable=false, captureSession=297, captureDelayMs=8704, capturePreambleMs=-1, triggerInData=false, sampleRate=16000, encoding=2, channelMask=1, data=0] false\n",
    "```\n",
    "\n",
    "3. 一级唤醒事件\n",
    "\n",
    "\n",
    "\n",
    "4. 二级唤醒事件\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 难点分析之：获取某一时刻在某一范围内的关键日志\n",
    "当然可以，下面是对代码中涉及的关键知识点的详细讲解：\n",
    "\n",
    "### 1. 正则表达式匹配时间戳\n",
    "\n",
    "```python\n",
    "timestamp_pattern = re.compile(r'(\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})')\n",
    "```\n",
    "\n",
    "- **正则表达式**：用于模式匹配字符串，这里使用 `\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}` 模式来匹配形如 \"MM-DD HH:MM:SS\" 的时间戳。\n",
    "  - `\\d{2}`：匹配两位数字。\n",
    "  - `-` 和 `:`：匹配特定的分隔符。\n",
    "  - 空格：匹配日期和时间之间的空格。\n",
    "- **re.compile**：将正则表达式编译成一个模式对象，以提高匹配效率。\n",
    "\n",
    "### 2. 解析时间戳\n",
    "\n",
    "```python\n",
    "def parse_log_timestamp(line):\n",
    "    match = timestamp_pattern.search(line)\n",
    "    if match:\n",
    "        timestamp_str = match.group(1)\n",
    "        return datetime.datetime.strptime(timestamp_str, '%m-%d %H:%M:%S').replace(year=datetime.datetime.now().year)\n",
    "    return None\n",
    "```\n",
    "\n",
    "- **re.search**：在字符串中查找正则表达式模式的第一个匹配项。\n",
    "- **match.group(1)**：获取匹配的第一个子组（即时间戳）。\n",
    "- **datetime.datetime.strptime**：将字符串解析为 `datetime` 对象。\n",
    "- **replace(year=...)**：因为日志中没有年份，默认使用当前年份。\n",
    "\n",
    "### 3. 二分查找\n",
    "\n",
    "```python\n",
    "def find_log_index(lines, target_time_str):\n",
    "    target_time = datetime.datetime.strptime(target_time_str, '%m-%d %H:%M:%S').replace(year=datetime.datetime.now().year)\n",
    "    \n",
    "    low, high = 0, len(lines) - 1\n",
    "    \n",
    "    while low <= high:\n",
    "        mid = (low + high) // 2\n",
    "        mid_time = parse_log_timestamp(lines[mid])\n",
    "        \n",
    "        if mid_time is None:\n",
    "            high = mid - 1\n",
    "            continue\n",
    "        \n",
    "        if mid_time < target_time:\n",
    "            low = mid + 1\n",
    "        elif mid_time > target_time:\n",
    "            high = mid - 1\n",
    "        else:\n",
    "            return mid\n",
    "    \n",
    "    return low\n",
    "```\n",
    "\n",
    "- **二分查找**：一种高效的查找算法，适用于已经排序的数组或列表。\n",
    "  - 通过不断将搜索范围减半，快速定位目标元素的位置。\n",
    "- **逻辑**：\n",
    "  - 初始化 `low` 和 `high` 指针。\n",
    "  - 在 `low <= high` 条件下进行循环：\n",
    "    - 计算中间位置 `mid`。\n",
    "    - 解析中间位置的时间戳 `mid_time`。\n",
    "    - 根据 `mid_time` 与 `target_time` 的比较结果，调整 `low` 或 `high` 指针。\n",
    "\n",
    "### 4. 提取目标时间范围的日志\n",
    "\n",
    "```python\n",
    "def extract_logs(filename, target_time_str, delta_minutes=3):\n",
    "    with open(filename, 'r', encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    target_index = find_log_index(lines, target_time_str)\n",
    "    \n",
    "    target_time = datetime.datetime.strptime(target_time_str, '%m-%d %H:%M:%S').replace(year=datetime.datetime.now().year)\n",
    "    delta = datetime.timedelta(minutes=delta_minutes)\n",
    "    \n",
    "    start_time = target_time - delta\n",
    "    end_time = target_time + delta\n",
    "\n",
    "    start_index = find_log_index(lines, start_time.strftime('%m-%d %H:%M:%S'))\n",
    "    end_index = find_log_index(lines, end_time.strftime('%m-%d %H:%M:%S'))\n",
    "    \n",
    "    start_index = max(0, start_index - 1)\n",
    "    end_index = min(len(lines) - 1, end_index + 1)\n",
    "    \n",
    "    return lines[start_index:end_index + 1]\n",
    "```\n",
    "\n",
    "- **读取文件内容**：\n",
    "  - `with open(filename, 'r', encoding=\"utf-8\") as file`：打开文件并读取所有行。\n",
    "- **目标时间和时间范围**：\n",
    "  - `datetime.datetime.strptime`：解析目标时间字符串为 `datetime` 对象。\n",
    "  - `datetime.timedelta`：创建一个时间间隔对象。\n",
    "  - 计算目标时间前后 `delta_minutes` 分钟的时间范围。\n",
    "- **二分查找**：\n",
    "  - 使用 `find_log_index` 找到目标时间及其前后时间范围的索引。\n",
    "- **处理边界情况**：\n",
    "  - 确保 `start_index` 和 `end_index` 在合法范围内。\n",
    "- **返回提取的日志行**：\n",
    "  - 返回目标时间范围内的日志行。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "\n",
    "# 正则表达式用于匹配时间戳，不包括毫秒部分\n",
    "timestamp_pattern = re.compile(r'(\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})')\n",
    "\n",
    "def parse_log_timestamp(line):\n",
    "    match = timestamp_pattern.search(line)\n",
    "    if match:\n",
    "        timestamp_str = match.group(1)\n",
    "        return datetime.datetime.strptime(timestamp_str, '%m-%d %H:%M:%S').replace(year=datetime.datetime.now().year)\n",
    "    return None\n",
    "\n",
    "# 二分判断\n",
    "def find_log_index(lines, target_time_str):\n",
    "    target_time = datetime.datetime.strptime(target_time_str, '%m-%d %H:%M:%S').replace(year=datetime.datetime.now().year)\n",
    "    \n",
    "    low, high = 0, len(lines) - 1\n",
    "    \n",
    "    while low <= high:\n",
    "        mid = (low + high) // 2\n",
    "        mid_time = parse_log_timestamp(lines[mid])\n",
    "        \n",
    "        if mid_time is None:\n",
    "            high = mid - 1\n",
    "            continue\n",
    "        \n",
    "        if mid_time < target_time:\n",
    "            low = mid + 1\n",
    "        elif mid_time > target_time:\n",
    "            high = mid - 1\n",
    "        else:\n",
    "            return mid\n",
    "    \n",
    "    return low\n",
    "\n",
    "# 二分查找确定关键日志\n",
    "def extract_logs(filename, target_time_str, pre_delta_minutes=3, post_delta_minutes=3):\n",
    "    with open(filename, 'r', encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    \n",
    "    target_time = datetime.datetime.strptime(target_time_str, '%m-%d %H:%M:%S').replace(year=datetime.datetime.now().year)\n",
    "    \n",
    "    # 前3分钟\n",
    "    pre_delta = datetime.timedelta(minutes=pre_delta_minutes)\n",
    "    # 后2分钟\n",
    "    post_delta = datetime.timedelta(minutes=post_delta_minutes)\n",
    "    \n",
    "    start_time = target_time - pre_delta\n",
    "    end_time = target_time + post_delta\n",
    "\n",
    "    start_index = find_log_index(lines, start_time.strftime('%m-%d %H:%M:%S'))\n",
    "    end_index = find_log_index(lines, end_time.strftime('%m-%d %H:%M:%S'))\n",
    "    \n",
    "    # 处理找不到确切位置的情况，确保 start_index 和 end_index 合法\n",
    "    start_index = max(0, start_index - 1)\n",
    "    end_index = min(len(lines) - 1, end_index + 1)\n",
    "    \n",
    "    # 提取目标日志范围\n",
    "    return lines[start_index:end_index + 1]\n",
    "\n",
    "# 示例使用\n",
    "log_filename = 'bugreport-goku-UKQ1.240116.001-2024-07-06-22-06-59.txt'\n",
    "target_time_str = '07-06 22:04:40'\n",
    "pre_delta_minutes = 3\n",
    "post_delta_minutes = 2\n",
    "\n",
    "logs = extract_logs(log_filename, target_time_str, pre_delta_minutes, post_delta_minutes)\n",
    "\n",
    "# 将提取的日志输出到一个新的文件\n",
    "with open('output.txt', 'w', encoding=\"utf-8\") as output_file:\n",
    "    output_file.writelines(logs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 难点分析之：有效日志关键词筛选\n",
    "包含voicetrigger项目的关键词在keyword.json文件中。规定时间内的日志再次进行筛选。\n",
    "1. 读取 JSON 文件\n",
    "2. 筛选日志中包含关键词的行\n",
    "3. 输出筛选后的日志"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "# 正则表达式用于匹配时间戳，不包括毫秒部分\n",
    "timestamp_pattern = re.compile(r'(\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})')\n",
    "\n",
    "def parse_log_timestamp(line):\n",
    "    match = timestamp_pattern.search(line)\n",
    "    if match:\n",
    "        timestamp_str = match.group(1)\n",
    "        return datetime.datetime.strptime(timestamp_str, '%m-%d %H:%M:%S').replace(year=datetime.datetime.now().year)\n",
    "    return None\n",
    "\n",
    "# 二分判断\n",
    "def find_log_index(lines, target_time_str):\n",
    "    target_time = datetime.datetime.strptime(target_time_str, '%m-%d %H:%M:%S').replace(year=datetime.datetime.now().year)\n",
    "    \n",
    "    low, high = 0, len(lines) - 1\n",
    "    \n",
    "    while low <= high:\n",
    "        mid = (low + high) // 2\n",
    "        mid_time = parse_log_timestamp(lines[mid])\n",
    "        \n",
    "        if mid_time is None:\n",
    "            high = mid - 1\n",
    "            continue\n",
    "        \n",
    "        if mid_time < target_time:\n",
    "            low = mid + 1\n",
    "        elif mid_time > target_time:\n",
    "            high = mid - 1\n",
    "        else:\n",
    "            return mid\n",
    "    \n",
    "    return low\n",
    "\n",
    "# 二分查找确定关键日志\n",
    "def extract_logs(filename, target_time_str, pre_delta_minutes=3, post_delta_minutes=2):\n",
    "    with open(filename, 'r', encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    target_index = find_log_index(lines, target_time_str)\n",
    "    \n",
    "    target_time = datetime.datetime.strptime(target_time_str, '%m-%d %H:%M:%S').replace(year=datetime.datetime.now().year)\n",
    "    \n",
    "    # 前3分钟\n",
    "    pre_delta = datetime.timedelta(minutes=pre_delta_minutes)\n",
    "    # 后2分钟\n",
    "    post_delta = datetime.timedelta(minutes=post_delta_minutes)\n",
    "    \n",
    "    start_time = target_time - pre_delta\n",
    "    end_time = target_time + post_delta\n",
    "\n",
    "    start_index = find_log_index(lines, start_time.strftime('%m-%d %H:%M:%S'))\n",
    "    end_index = find_log_index(lines, end_time.strftime('%m-%d %H:%M:%S'))\n",
    "    \n",
    "    # 处理找不到确切位置的情况，确保 start_index 和 end_index 合法\n",
    "    start_index = max(0, start_index - 1)\n",
    "    end_index = min(len(lines) - 1, end_index + 1)\n",
    "    \n",
    "    # 提取目标日志范围\n",
    "    return lines[start_index:end_index + 1]\n",
    "\n",
    "# 筛选包含关键词的日志\n",
    "def filter_logs_by_keywords(logs, keywords):\n",
    "    filtered_logs = []\n",
    "    for line in logs:\n",
    "        if any(keyword in line for keyword in keywords):\n",
    "            filtered_logs.append(line)\n",
    "    return filtered_logs\n",
    "\n",
    "# 示例使用\n",
    "log_filename = 'bugreport-goku-UKQ1.240116.001-2024-07-06-22-06-59.txt'\n",
    "target_time_str = '07-06 20:29:45'\n",
    "pre_delta_minutes = 3\n",
    "post_delta_minutes = 2\n",
    "\n",
    "# 提取关键时间范围内的日志\n",
    "logs = extract_logs(log_filename, target_time_str, pre_delta_minutes, post_delta_minutes)\n",
    "\n",
    "# 读取关键词JSON文件\n",
    "with open('voicetrigger_keywords.json', 'r', encoding=\"utf-8\") as file:\n",
    "    keywords_data = json.load(file)\n",
    "keywords = keywords_data['keywords']\n",
    "\n",
    "# 筛选包含关键词的日志\n",
    "filtered_logs = filter_logs_by_keywords(logs, keywords)\n",
    "\n",
    "# 将筛选后的日志输出到一个新的文件\n",
    "with open('filtered_output.txt', 'w', encoding=\"utf-8\") as output_file:\n",
    "    output_file.writelines(filtered_logs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分析有误，不能按照二分的方式去分析\n",
    "所以先按照全部遍历的方式，然后去优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "\n",
    "# 正则表达式用于匹配时间戳，不包括毫秒部分\n",
    "timestamp_pattern = re.compile(r'(\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})')\n",
    "\n",
    "def parse_log_timestamp(line):\n",
    "    match = timestamp_pattern.search(line)\n",
    "    if match:\n",
    "        timestamp_str = match.group(1)\n",
    "        return datetime.datetime.strptime(timestamp_str, '%m-%d %H:%M:%S').replace(year=datetime.datetime.now().year)\n",
    "    return None\n",
    "\n",
    "def extract_logs(filename, target_time_str, pre_delta_minutes=3, post_delta_minutes=2):\n",
    "    with open(filename, 'r', encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    target_time = datetime.datetime.strptime(target_time_str, '%m-%d %H:%M:%S').replace(year=datetime.datetime.now().year)\n",
    "    \n",
    "    # 前3分钟\n",
    "    pre_delta = datetime.timedelta(minutes=pre_delta_minutes)\n",
    "    # 后2分钟\n",
    "    post_delta = datetime.timedelta(minutes=post_delta_minutes)\n",
    "    \n",
    "    start_time = target_time - pre_delta\n",
    "    end_time = target_time + post_delta\n",
    "\n",
    "    result_lines = []\n",
    "    within_time_range = False\n",
    "\n",
    "    for line in lines:\n",
    "        line_time = parse_log_timestamp(line)\n",
    "        \n",
    "        if line_time:\n",
    "            within_time_range = start_time <= line_time <= end_time\n",
    "            if within_time_range:\n",
    "                result_lines.append(line)\n",
    "\n",
    "    return result_lines\n",
    "\n",
    "# 示例使用\n",
    "log_filename = 'bugreport-goku-UKQ1.240116.001-2024-07-06-22-06-59.txt'\n",
    "target_time_str = '07-06 22:04:40'\n",
    "pre_delta_minutes = 3\n",
    "post_delta_minutes = 2\n",
    "\n",
    "logs = extract_logs(log_filename, target_time_str, pre_delta_minutes, post_delta_minutes)\n",
    "\n",
    "# 筛选包含关键词的日志\n",
    "def filter_logs_by_keywords(logs, keywords):\n",
    "    filtered_logs = []\n",
    "    for line in logs:\n",
    "        if any(keyword in line for keyword in keywords):\n",
    "            filtered_logs.append(line)\n",
    "    return filtered_logs\n",
    "\n",
    "\n",
    "# 读取关键词JSON文件\n",
    "with open('voicetrigger_keywords.json', 'r', encoding=\"utf-8\") as file:\n",
    "    keywords_data = json.load(file)\n",
    "keywords = keywords_data['keywords']\n",
    "\n",
    "\n",
    "# # 将提取的日志输出到一个新的文件\n",
    "with open('outputNew.txt', 'w', encoding=\"utf-8\") as output_file:\n",
    "    output_file.writelines(logs)\n",
    "\n",
    "# 筛选包含关键词的日志\n",
    "filtered_logs = filter_logs_by_keywords(logs, keywords)\n",
    "\n",
    "# 将筛选后的日志输出到一个新的文件\n",
    "with open('filtered_output.txt', 'w', encoding=\"utf-8\") as output_file:\n",
    "    output_file.writelines(filtered_logs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化之后的日志筛选\n",
    "1. 直接每次分析每2000行的第一条和最后一条日志的时间。如果其中一条日志满足，就直接将这1000行日志最为有效日志存起来；\n",
    "2. 对于遍历大数量的日志确实能提高效率，比逐行遍历每一条日志提高效率：\n",
    "    1. 逐行遍历时间复杂度为`O(n)`\n",
    "    2. 间隔2000行粗略筛选，时间复杂度为`O(n/2000)`\n",
    "3. 为什么是选择间隔1000行。因为我发现一段有效且连续在一分钟内的日志一定大于1000行。\n",
    "4. 粗略筛选之后还需要继续细致筛选，因此粗略筛选不用非常详细。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "\n",
    "# 正则表达式用于匹配时间戳，不包括毫秒部分\n",
    "timestamp_pattern = re.compile(r'(\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})')\n",
    "\n",
    "def parse_log_timestamp(line):\n",
    "    match = timestamp_pattern.search(line)\n",
    "    if match:\n",
    "        timestamp_str = match.group(1)\n",
    "        return datetime.datetime.strptime(timestamp_str, '%m-%d %H:%M:%S').replace(year=datetime.datetime.now().year)\n",
    "    return None\n",
    "\n",
    "def extract_logs(filename, target_time_str, pre_delta_minutes=3, post_delta_minutes=2):\n",
    "    with open(filename, 'r', encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    target_time = datetime.datetime.strptime(target_time_str, '%m-%d %H:%M:%S').replace(year=datetime.datetime.now().year)\n",
    "    # 前3分钟\n",
    "    pre_delta = datetime.timedelta(minutes=pre_delta_minutes)\n",
    "    # 后2分钟\n",
    "    post_delta = datetime.timedelta(minutes=post_delta_minutes)\n",
    "    \n",
    "    start_time = target_time - pre_delta\n",
    "    end_time = target_time + post_delta\n",
    "\n",
    "    result_lines = []\n",
    "    step = 2000\n",
    "    total_lines = len(lines)\n",
    "    i = 0\n",
    "\n",
    "    while i < total_lines:\n",
    "        # 获取当前位置和1000行后的时间戳\n",
    "        current_time = parse_log_timestamp(lines[i])\n",
    "        next_i = min(i + step, total_lines - 1)\n",
    "        next_time = parse_log_timestamp(lines[next_i])\n",
    "\n",
    "        if (current_time and start_time <= current_time <= end_time) or (next_time and start_time <= next_time <= end_time):\n",
    "            \n",
    "            result_lines.extend(lines[i:next_i + 1])\n",
    "            \n",
    "        i += step\n",
    "\n",
    "    return result_lines\n",
    "\n",
    "# 示例使用\n",
    "log_filename = 'bugreport-goku-UKQ1.240116.001-2024-07-06-22-06-59.txt'\n",
    "target_time_str = '07-06 22:04:40'\n",
    "pre_delta_minutes = 3\n",
    "post_delta_minutes = 2\n",
    "logs = extract_logs(log_filename, target_time_str, pre_delta_minutes, post_delta_minutes)\n",
    "\n",
    "\n",
    "# 筛选包含关键词的日志\n",
    "def filter_logs_by_keywords(logs, keywords):\n",
    "    filtered_logs = []\n",
    "    for line in logs:\n",
    "        if any(keyword in line for keyword in keywords):\n",
    "            filtered_logs.append(line)\n",
    "    return filtered_logs\n",
    "\n",
    "\n",
    "# 读取关键词JSON文件\n",
    "with open('voicetrigger_keywords.json', 'r', encoding=\"utf-8\") as file:\n",
    "    keywords_data = json.load(file)\n",
    "keywords = keywords_data['keywords']\n",
    "\n",
    "# 将提取的日志输出到一个新的文件\n",
    "# with open('output.txt', 'w', encoding=\"utf-8\") as output_file:\n",
    "#     output_file.writelines(logs)\n",
    "\n",
    "# 筛选包含关键词的日志\n",
    "filtered_logs = filter_logs_by_keywords(logs, keywords)\n",
    "\n",
    "# 将筛选后的日志输出到一个新的文件\n",
    "with open('new_filtered_output.txt', 'w', encoding=\"utf-8\") as output_file:\n",
    "    output_file.writelines(filtered_logs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分析规定时间段的一级事件、二级事件\n",
    "\n",
    "有一个很有意思的地方，只有收到一级唤醒事件，才能得到唤醒方案和相关算法。\n",
    "\n",
    "### 1. 未收到一级唤醒事件\n",
    "四种方案通用\n",
    "未查询到关键日志点‘onRecognition:’，则标记为：未接收到一级事件\n",
    "```bash\n",
    "(高通/自研E) D/SVA-LegacyWakeupSession - onRecognition: recognitionEvent = KeyphraseRecognitionEvent [keyphraseExtras=[KeyphraseRecognitionExtra [id=0, recognitionModes=3, coarseConfidenceLevel=98, confidenceLevels=[ConfidenceLevel [userId=1, confidenceLevel=14]]]], status=0, soundModelHandle=1, captureAvailable=true, captureSession=25, captureDelayMs=0, capturePreambleMs=0, triggerInData=false, sampleRate=16000, encoding=2, channelMask=16, data=8944]\n",
    "(MTK) V/VoiceTriggerManager - onRecognition: GenericRecognitionEvent ::RecognitionEvent [status=0, soundModelHandle=-1, captureAvailable=false, captureSession=-1, captureDelayMs=-1, capturePreambleMs=-1, triggerInData=false, sampleRate=16000, encoding=2, channelMask=1, data=0]\n",
    "(MTK自研) V/MTKVoiceTriggerManager - onRecognition: GenericRecognitionEvent ::RecognitionEvent [status=0, soundModelHandle=-1, captureAvailable=false, captureSession=297, captureDelayMs=8704, capturePreambleMs=-1, triggerInData=false, sampleRate=16000, encoding=2, channelMask=1, data=0] false\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 2. 一级唤醒事件失败\n",
    "\n",
    "只适用于高通方案\n",
    "查询到关键日志点‘final_result=false’，则标记为：未通过一级\n",
    "```bash\n",
    "final_result=false, first_stage_detect= true, cnn_detect= false, vop_detect=false；---唤醒词、声纹均验证不通过（与柳军确认，cnn/vop检测不分先后，任意一个false，就不会再去验证另一个）\n",
    "final_result=false, first_stage_detect= true, cnn_detect= false, vop_detect=true；---唤醒词验证不通过\n",
    "final_result=false, first_stage_detect=true, cnn_detect=true, vop_detect=false；---声纹检测不通过\n",
    "final_result= true, first_stage_detect=true, cnn_detect= true, vop_detect=true；---验证通过\n",
    "```\n",
    "\n",
    "\n",
    "### 3. 非正常一级事件\n",
    "查询到一次‘onRecognition: recognitionEvent’事件后，发现其采样率‘sampleRate=0’、数据都有问题，则输出：非正常一级事件\n",
    "```bash\n",
    "D/SVA-LegacyWakeupSession - onRecognition: recognitionEvent = \n",
    "KeyphraseRecognitionEvent [keyphraseExtras=[], status=1, \n",
    "soundModelHandle=2, captureAvailable=true, captureSession=0, \n",
    "captureDelayMs=0, capturePreambleMs=0, triggerInData=false, \n",
    "sampleRate=0, encoding=0, channelMask=0, data=0]\n",
    "```\n",
    "\n",
    "\n",
    "### 3. 二级唤醒事件失败\n",
    "#### 3.1 二级引擎不通过\n",
    "- 日志定位\n",
    "**该日志点目前仅自研方案存在**\n",
    "    - 先定位到：‘PhraseWakeupResult’；\n",
    "    - 再定位到：‘isVoconWakeupPassed’和‘isVBPassed’\n",
    "该条日志共四种情况：\n",
    "1. 若isVoconWakeupPassed=true、isVBPassed=true，则通过二级引擎\n",
    "```bash\n",
    "D/MIXWVPCallback - PhraseWakeupResult{mVoconPhrase='小爱同学', \n",
    "isVoconWakeupPassed=true, mWakeupStartTime=-1, mWakeupEndTime=-1, \n",
    "mScore=23.421255, isAec=false, isVBPassed=true, mWakeupResultStart=1084, \n",
    "mWakeupResultEnd=1232}\n",
    "```\n",
    "\n",
    "2. 若isVoconWakeupPassed=false、isVBPassed=true，则标记为：未通过二级唤醒引擎\n",
    "```bash\n",
    "D/MIXWVPCallback - PhraseWakeupResult{mVoconPhrase='小爱同学', \n",
    "isVoconWakeupPassed=false, mWakeupStartTime=-1, mWakeupEndTime=-1, \n",
    "mScore=0.0, isAec=false, isVBPassed=true, mWakeupResultStart=0, \n",
    "mWakeupResultEnd=0}\n",
    "```\n",
    "\n",
    "3. 若isVoconWakeupPassed=true、isVBPassed=false，则标记为：未通过二级声纹引擎\n",
    "```bash\n",
    "D/MIXWVPCallback - PhraseWakeupResult{mVoconPhrase='小爱同学', \n",
    "isVoconWakeupPassed=true, mWakeupStartTime=-1, mWakeupEndTime=-1, \n",
    "mScore=29.038092, isAec=false, isVBPassed=false, mWakeupResultStart=552, \n",
    "mWakeupResultEnd=641}\n",
    "```\n",
    "\n",
    "4. 若isVoconWakeupPassed=true、isVBPassed=false，则标记为：未通过二级唤醒引擎及声纹引擎\n",
    "```bash\n",
    "D/MIXWVPCallback - PhraseWakeupResult{mVoconPhrase='小爱同学', \n",
    "isVoconWakeupPassed=false, mWakeupStartTime=-1, mWakeupEndTime=-1, \n",
    "mScore=0.0, isAec=false, isVBPassed=false, mWakeupResultStart=0, \n",
    "mWakeupResultEnd=0}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
